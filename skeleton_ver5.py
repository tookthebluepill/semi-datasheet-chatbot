import streamlit as st
import os
from dotenv import load_dotenv
import nest_asyncio

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_classic.schema import Document

# í‘œì¤€ LangChain ëª¨ë“ˆ (ë©”ëª¨ë¦¬ & ì²´ì¸)
from langchain_classic.memory import ConversationBufferMemory
from langchain_classic.chains import ConversationalRetrievalChain

# [í•µì‹¬] LlamaParse & PDF Viewer
from llama_parse import LlamaParse
from streamlit_pdf_viewer import pdf_viewer

# ë¹„ë™ê¸° ì¶©ëŒ ë°©ì§€ (LlamaParse í•„ìˆ˜)
nest_asyncio.apply()

# API í‚¤ ì„¤ì •
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")
if not os.getenv("GOOGLE_API_KEY"):
    st.error("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

if not os.getenv("LLAMA_CLOUD_API_KEY"):
    st.error("LLAMA_CLOUD_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")

# [UI ì„¤ì •] ì™€ì´ë“œ ëª¨ë“œ (í™”ë©´ ë¶„í• ì„ ìœ„í•´ í•„ìˆ˜)
st.set_page_config(layout="wide", page_title="ë°˜ë„ì²´ ë°ì´í„°ì‹œíŠ¸ Chatbot")

st.title("ë°˜ë„ì²´ ë°ì´í„°ì‹œíŠ¸ Chatbot")

# 2. ë²¡í„° DB ìƒì„± í•¨ìˆ˜ (ìºì‹± ì ìš©)
@st.cache_resource
def get_vectorstore(file_path):
    try:
        # [1] LlamaParseë¡œ PDF ì½ê¸° (Markdown ë³€í™˜)
        parser = LlamaParse(
            api_key=LLAMA_CLOUD_API_KEY,
            result_type="markdown",
            verbose=True
        )
        llama_documents = parser.load_data(file_path)
        
        if not llama_documents:
            st.error("PDFë¥¼ ì½ì—ˆìœ¼ë‚˜ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None

        # [2] LangChain í¬ë§·ìœ¼ë¡œ ë³€í™˜
        langchain_documents = []
        for doc in llama_documents:
            # LlamaParseì˜ ë©”íƒ€ë°ì´í„°(í˜ì´ì§€ ë²ˆí˜¸ ë“±)ë¥¼ ë³µì‚¬
            doc_metadata = doc.metadata.copy()
            doc_metadata["source"] = file_path
            
            langchain_documents.append(
                Document(page_content=doc.text, metadata=doc_metadata)
            )
        
        # [3] ì²­í‚¹ (Chunking)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        texts = text_splitter.split_documents(langchain_documents)
        
        if not texts:
            st.error("í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # [4] ì„ë² ë”© & ë²¡í„° ì €ì¥
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore = FAISS.from_documents(texts, embeddings)
        return vectorstore
        
    except Exception as e:
        st.error(f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# 3. ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

# í˜„ì¬ í˜ì´ì§€ ìƒíƒœ (PDF ë·°ì–´ ì œì–´ìš©)
if "current_page" not in st.session_state:
    st.session_state.current_page = 1

# ì‚¬ì´ë“œë°” & íŒŒì¼ ì—…ë¡œë“œ
with st.sidebar:
    st.header("ğŸ“‚ ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ë°ì´í„°ì‹œíŠ¸ PDF ì—…ë¡œë“œ", type="pdf")

# ë©”ì¸ ë¡œì§ ì‹œì‘
if uploaded_file is not None:
    file_path = "temp.pdf"
    binary_data = uploaded_file.getvalue()
    
    # ì„ì‹œ íŒŒì¼ ì €ì¥
    with open(file_path, "wb") as f:
        f.write(binary_data)
    
    # VectorStore ë¡œë”© (ì„¸ì…˜ì— ì €ì¥í•˜ì—¬ Rerun ì‹œ ì¬ë¶„ì„ ë°©ì§€)
    if "vectorstore" not in st.session_state:
        with st.spinner("LlamaParseê°€ í‘œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (ìµœì´ˆ 1íšŒ, ì‹œê°„ ì†Œìš”ë¨)"):
            vs = get_vectorstore(file_path)
            if vs:
                st.session_state.vectorstore = vs
                st.success("ë¶„ì„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
            else:
                st.stop() # ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨

    # í™”ë©´ 50:50 ë¶„í• 
    col1, col2 = st.columns([1, 1])

    # [Right] PDF Viewer (ë¨¼ì € ë°°ì¹˜)
    with col2:
        st.info(f"ğŸ“„ ë¬¸ì„œ ë·°ì–´ (í˜„ì¬ í˜ì´ì§€: {st.session_state.current_page})")
        
        # [í•µì‹¬] keyë¥¼ ê³ ì •("pdf_viewer")í•˜ê³ , pages_to_renderë¥¼ ì„¸ì…˜ ë³€ìˆ˜ë¡œ ì œì–´
        # í˜ì´ì§€ê°€ ë°”ë€Œë©´ st.rerun()ì´ ë°œìƒí•˜ì—¬ ì´ ì½”ë“œê°€ ë‹¤ì‹œ ì‹¤í–‰ë˜ê³ , ë·°ì–´ê°€ ê°±ì‹ ë¨
        pdf_viewer(
            input=binary_data,
            width=700,
            height=800,
            pages_to_render=[st.session_state.current_page],
            key=f"pdf_viewer_page_{st.session_state.current_page}"
        )

    # [Left] Chat Interface
    with col1:
        st.subheader("ğŸ’¬ AI ì—”ì§€ë‹ˆì–´")
        
        # ì±„íŒ… ê¸°ë¡ ì»¨í…Œì´ë„ˆ
        chat_container = st.container(height=600)
        with chat_container:
            for message in st.session_state.chat_history:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

        # ì§ˆë¬¸ ì…ë ¥ ì²˜ë¦¬
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            # 1. ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(prompt)
            st.session_state.chat_history.append({"role": "user", "content": prompt})

            # 2. AI ë‹µë³€ ìƒì„±
            with chat_container:
                with st.chat_message("assistant"):
                    with st.spinner("ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
                        llm = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)
                        
                        # ì²´ì¸ ìƒì„±
                        qa_chain = ConversationalRetrievalChain.from_llm(
                            llm=llm,
                            retriever=st.session_state.vectorstore.as_retriever(),
                            memory=st.session_state.memory,
                            return_source_documents=True
                        )
                        
                        # ì‹¤í–‰
                        result = qa_chain.invoke({"question": prompt})
                        response = result["answer"]
                        source_docs = result['source_documents']
                        
                        st.markdown(response)
                        
                        # í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ ë° ì´ë™ ëŒ€ìƒ ê²°ì •
                        target_page = st.session_state.current_page # ê¸°ë³¸ê°’: ìœ ì§€
                        
                        if source_docs:
                            # ë””ë²„ê¹…ìš©: AIê°€ ì°¾ì€ ë©”íƒ€ë°ì´í„° í™•ì¸ (ê°œë°œ ì™„ë£Œ í›„ ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥)
                            # st.toast(f"ë©”íƒ€ë°ì´í„°: {source_docs[0].metadata}", icon="ğŸ”")
                            
                            try:
                                best_doc = source_docs[0]
                                # LlamaParse ìš°ì„ ìˆœìœ„: page_label(ë¬¸ì„œ ë²ˆí˜¸) -> page(ì¸ë±ìŠ¤)
                                page_label = best_doc.metadata.get("page_label")
                                page_num = best_doc.metadata.get("page")
                                
                                if page_label:
                                    target_page = int(page_label)
                                elif page_num is not None:
                                    target_page = int(page_num) + 1 # 0-based index ë³´ì •
                                
                                # ì•ˆì „ì¥ì¹˜: 1í˜ì´ì§€ ë¯¸ë§Œ ë°©ì§€
                                target_page = max(1, target_page)
                                
                            except Exception as e:
                                print(f"í˜ì´ì§€ ì¶”ì¶œ ì—ëŸ¬: {e}")
                                # ì—ëŸ¬ ì‹œ í˜ì´ì§€ ì´ë™ ì•ˆ í•¨

                            # ê·¼ê±° ë¬¸ì„œ ì•„ì½”ë””ì–¸ í‘œì‹œ
                            with st.expander("ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš©"):
                                for doc in source_docs:
                                    p_info = doc.metadata.get("page_label") or doc.metadata.get("page")
                                    st.caption(f"[Page {p_info}] {doc.page_content[:200]}...")

            # 3. ëŒ€í™” ê¸°ë¡ ì €ì¥
            st.session_state.chat_history.append({"role": "assistant", "content": response})
            
            # 4. [Rerun íŠ¸ë¦¬ê±°] í˜ì´ì§€ê°€ ë³€ê²½ë˜ì–´ì•¼ í•œë‹¤ë©´, ìƒíƒœ ì—…ë°ì´íŠ¸ í›„ ì¦‰ì‹œ Rerun!
            if target_page != st.session_state.current_page:
                st.session_state.current_page = target_page
                st.rerun()