import streamlit as st
import os
import hashlib
from dotenv import load_dotenv
import nest_asyncio

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_classic.schema import Document

# í‘œì¤€ LangChain ëª¨ë“ˆ
from langchain_classic.memory import ConversationBufferMemory
from langchain_classic.chains import ConversationalRetrievalChain

# PDF ì²˜ë¦¬
from llama_parse import LlamaParse
from streamlit_pdf_viewer import pdf_viewer

# ë¹„ë™ê¸° ì¶©ëŒ ë°©ì§€
nest_asyncio.apply()

# í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
load_dotenv() # .env íŒŒì¼ ë¡œë“œ

# API í‚¤
if not os.getenv("GOOGLE_API_KEY"):
    st.error("âŒ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

if not os.getenv("LLAMA_CLOUD_API_KEY"):
    st.error("âŒ LLAMA_CLOUD_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")

# UI ì„¤ì •
st.set_page_config(layout="wide", page_title="Semi-Datasheet-Chatbot")
st.title("âš¡ ë°˜ë„ì²´ ë°ì´í„°ì‹œíŠ¸ Chatbot (Pro Ver.)")

# ìœ í‹¸: íŒŒì¼ í•´ì‹œ ìƒì„± (ìºì‹± í‚¤ë¡œ ì‚¬ìš©)
def get_file_hash(file_bytes):
    return hashlib.md5(file_bytes).hexdigest()

# VectorStore ìƒì„±
@st.cache_resource
def get_vectorstore(file_path, file_hash):
    """
    1ìˆœìœ„: ë¡œì»¬ì— ì €ì¥ëœ FAISS DBê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë¡œë“œ (ê°€ì¥ ë¹ ë¦„)
    2ìˆœìœ„: íŒŒì‹±ëœ Markdown íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ í›„ ì„ë² ë”© (LlamaParse ì ˆì•½)
    3ìˆœìœ„: ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ LlamaParse API í˜¸ì¶œ -> Markdown ì €ì¥ -> FAISS ì €ì¥
    """
    
    # ìºì‹œ í´ë” ê²½ë¡œ ì„¤ì •
    faiss_cache_dir = os.path.join("faiss_cache", file_hash)
    parsed_cache_path = os.path.join("parsed_cache", f"{file_hash}.md")
    
    # ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ (ë¡œë”©ê³¼ ìƒì„± ëª¨ë‘ì— í•„ìš”)
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # ì´ë¯¸ ë§Œë“¤ì–´ë‘” FAISS DBê°€ ìˆëŠ”ì§€ í™•ì¸
    if os.path.exists(faiss_cache_dir):
        if os.path.exists(os.path.join(faiss_cache_dir, "index.faiss")):
            st.info(f"ìºì‹œëœ ë²¡í„° DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
            # allow_dangerous_deserialization=TrueëŠ” ë¡œì»¬ì—ì„œ ë‚´ê°€ ë§Œë“  íŒŒì¼ì„ ë¯¿ëŠ”ë‹¤ëŠ” ëœ»
            vectorstore = FAISS.load_local(
                faiss_cache_dir, 
                embeddings, 
                allow_dangerous_deserialization=True
            )
            return vectorstore

    # FAISSê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸(Markdown)ë¥¼ ì¤€ë¹„í•´ì•¼ í•¨
    markdown_text = ""
    llama_documents = []

    # íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ìºì‹œê°€ ìˆëŠ”ì§€ í™•ì¸
    if os.path.exists(parsed_cache_path):
        st.info(f"ì €ì¥ëœ íŒŒì‹± í…ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
        with open(parsed_cache_path, "r", encoding="utf-8") as f:
            markdown_text = f.read()
        llama_documents = [Document(page_content=markdown_text, metadata={"source": file_path})]
    
    else:
        # ìºì‹œê°€ ì „í˜€ ì—†ìœ¼ë©´ LlamaParse API ì‹¤í–‰
        try:
            st.info("LlamaCloudì—ì„œ ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (ìµœì´ˆ 1íšŒ, í† í° ì‚¬ìš©)")
            parser = LlamaParse(
                api_key=LLAMA_CLOUD_API_KEY,
                result_type="markdown",
                verbose=True
            )
            parsed_docs = parser.load_data(file_path)
            
            if not parsed_docs:
                st.error("PDF ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return None
            
            # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° ë° ì €ì¥
            markdown_text = "\n\n".join([doc.text for doc in parsed_docs])
            
            if not os.path.exists("parsed_cache"):
                os.makedirs("parsed_cache")
                
            with open(parsed_cache_path, "w", encoding="utf-8") as f:
                f.write(markdown_text)
                
            llama_documents = parsed_docs

        except Exception as e:
            st.error(f"LlamaParse ì˜¤ë¥˜: {e}")
            return None

    # í…ìŠ¤íŠ¸ -> ì²­í‚¹ -> ì„ë² ë”© -> FAISS
    st.info("í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ ë° ì €ì¥ ì¤‘ì…ë‹ˆë‹¤...")
    
    # LangChain Document í˜•ì‹ ì •ë¦¬
    langchain_documents = []
    if isinstance(llama_documents[0], Document):
         langchain_documents = llama_documents
    else:
        for doc in llama_documents:
            doc_metadata = doc.metadata.copy()
            doc_metadata["source"] = file_path
            langchain_documents.append(
                Document(page_content=doc.text, metadata=doc_metadata)
            )

    # ì²­í‚¹ (Chunking)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(langchain_documents)
    
# FAISS ìƒì„±
    vectorstore = FAISS.from_documents(texts, embeddings)
    
    # ì™„ì„±ëœ FAISS DBë¥¼ í†µì§¸ë¡œ ì €ì¥ (ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´)
    vectorstore.save_local(faiss_cache_dir)
    st.success("ë²¡í„° DB ì €ì¥ ì™„ë£Œ! ë‹¤ìŒë¶€í„°ëŠ” ì¦‰ì‹œ ë¡œë”©ë©ë‹ˆë‹¤.")
    
    return vectorstore

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

if "current_page" not in st.session_state:
    st.session_state.current_page = 1

# ì‚¬ì´ë“œë°” (íŒŒì¼ ì—…ë¡œë“œ)
with st.sidebar:
    st.header("ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ë°ì´í„°ì‹œíŠ¸ PDF ì—…ë¡œë“œ", type="pdf")

# ë©”ì¸ ì–´í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§
if uploaded_file is not None:
    # 1. íŒŒì¼ í•´ì‹œ ê³„ì‚° (ê³ ìœ  ID)
    binary_data = uploaded_file.getvalue()
    file_hash = get_file_hash(binary_data)
    
    # 2. ì„ì‹œ íŒŒì¼ ì €ì¥
    file_path = f"temp_{file_hash}.pdf"
    with open(file_path, "wb") as f:
        f.write(binary_data)

    # 3. VectorStore ë¡œë“œ (ì„¸ì…˜ì— ì—†ë‹¤ë©´ ìƒì„±)
    # file_hashê°€ ë°”ë€Œë©´(ë‹¤ë¥¸ íŒŒì¼) ë‹¤ì‹œ ë¡œë“œí•¨
    if "vectorstore" not in st.session_state or st.session_state.get("current_file_hash") != file_hash:
        vs = get_vectorstore(file_path, file_hash)
        
        if vs is None:
            st.stop()
            
        st.session_state.vectorstore = vs
        st.session_state.current_file_hash = file_hash # í˜„ì¬ íŒŒì¼ í•´ì‹œ ì €ì¥

    # í™”ë©´ ë¶„í•  (ì™¼ìª½: ì±„íŒ… / ì˜¤ë¥¸ìª½: PDF)
    col1, col2 = st.columns([1, 1])

    # [Right] PDF Viewer
    with col2:
        st.info(f"ë¬¸ì„œ ë·°ì–´ (Page: {st.session_state.current_page})")
        
        # keyë¥¼ "pdf_viewer"ë¡œ ê³ ì •í•˜ê³ , pages_to_renderë¥¼ ì„¸ì…˜ ìƒíƒœë¡œ ì œì–´
        # Rerun ë  ë•Œë§ˆë‹¤ ì´ ë¶€ë¶„ì´ ë‹¤ì‹œ ì‹¤í–‰ë˜ë©° í˜ì´ì§€ê°€ ê°±ì‹ ë¨
        pdf_viewer(
            input=binary_data,
            width=700,
            height=800,
            pages_to_render=[st.session_state.current_page],
            key="pdf_viewer"
        )

    # [Left] Chat Interface
    with col1:
        st.subheader("ğŸ’¬ AI ì—”ì§€ë‹ˆì–´")

        # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
        chat_container = st.container(height=600)
        with chat_container:
            for message in st.session_state.chat_history:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

        # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            # UI ì—…ë°ì´íŠ¸
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(prompt)
            st.session_state.chat_history.append({"role": "user", "content": prompt})

            # ë‹µë³€ ìƒì„±
            with chat_container:
                with st.chat_message("assistant"):
                    with st.spinner("ë¶„ì„ ì¤‘..."):
                        # [ì•ˆì •ì„±] 429 ì—ëŸ¬ ë°©ì§€ìš© max_retries ì¶”ê°€
                        llm = ChatGoogleGenerativeAI(
                            model="gemini-1.5-flash",
                            temperature=0,
                            max_retries=2
                        )

                        qa_chain = ConversationalRetrievalChain.from_llm(
                            llm=llm,
                            retriever=st.session_state.vectorstore.as_retriever(),
                            memory=st.session_state.memory,
                            return_source_documents=True
                        )

                        result = qa_chain.invoke({"question": prompt})
                        response = result["answer"]
                        source_docs = result["source_documents"]

                        st.markdown(response)

                        # í˜ì´ì§€ ì´ë™ ë¡œì§
                        target_page = st.session_state.current_page
                        
                        if source_docs:
                            # ë””ë²„ê·¸ìš© (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
                            # st.toast(f"ë©”íƒ€ë°ì´í„°: {source_docs[0].metadata}")
                            try:
                                best_doc = source_docs[0]
                                page_label = best_doc.metadata.get("page_label")
                                page_num = best_doc.metadata.get("page")

                                if page_label:
                                    target_page = int(page_label)
                                elif page_num is not None:
                                    target_page = int(page_num) + 1 # 0-based -> 1-based

                                target_page = max(1, target_page) # ìµœì†Œ 1í˜ì´ì§€ ë³´ì¥

                            except Exception as e:
                                print(f"í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

                            # ê·¼ê±° ë¬¸ì„œ í‘œì‹œ
                            with st.expander("ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš©"):
                                for doc in source_docs:
                                    p = doc.metadata.get("page_label") or doc.metadata.get("page")
                                    st.caption(f"[Page {p}] {doc.page_content[:200]}...")

            # AI ì‘ë‹µ ì €ì¥
            st.session_state.chat_history.append({"role": "assistant", "content": response})

            # í˜ì´ì§€ê°€ ë‹¬ë¼ì¡Œìœ¼ë©´ Rerun (ë·°ì–´ ê°±ì‹ )
            if target_page != st.session_state.current_page:
                st.session_state.current_page = target_page
                st.rerun()