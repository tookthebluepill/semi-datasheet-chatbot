import streamlit as st
import os
import hashlib
import json
import re  # [ì¶”ê°€] ì •ê·œí‘œí˜„ì‹ (í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œìš©)
from dotenv import load_dotenv
import nest_asyncio

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_classic.schema import Document

# í‘œì¤€ LangChain ëª¨ë“ˆ
from langchain_classic.memory import ConversationBufferMemory
from langchain_classic.chains import ConversationalRetrievalChain

# PDF ì²˜ë¦¬
from llama_parse import LlamaParse
from streamlit_pdf_viewer import pdf_viewer

# ë¹„ë™ê¸° ì¶©ëŒ ë°©ì§€
nest_asyncio.apply()

# í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
load_dotenv()

if not os.getenv("GOOGLE_API_KEY"):
    st.error("âŒ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

if not os.getenv("LLAMA_CLOUD_API_KEY"):
    st.error("âŒ LLAMA_CLOUD_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")

st.set_page_config(layout="wide", page_title="Semi-Datasheet-Chatbot")
st.title("ë°˜ë„ì²´ ë°ì´í„°ì‹œíŠ¸ Chatbot (Pro Ver.)")

# ìœ í‹¸ 1: íŒŒì¼ í•´ì‹œ (ìºì‹± í‚¤)
def get_file_hash(file_bytes):
    return hashlib.md5(file_bytes).hexdigest()

# ìœ í‹¸ 2: ì•ˆì „í•œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ (Regex ì ìš©)
def get_safe_page_number(page_value, default=1):
    """
    'Page 3', '3/10', 'iv' ë“± ë‹¤ì–‘í•œ í˜•ì‹ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
    """
    if page_value is None:
        return default
    
    # ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ìˆ«ì íƒìƒ‰
    s_val = str(page_value)
    match = re.search(r"(\d+)", s_val)
    
    if match:
        return int(match.group(1))
    return default

# VectorStore ìƒì„± (JSON ìºì‹± + í˜ì´ì§€ ë³´ì¡´)
@st.cache_resource
def get_vectorstore(file_path, file_hash):
    # í´ë” ì¤€ë¹„
    faiss_cache_dir = os.path.join("faiss_cache", file_hash)
    # MD ëŒ€ì‹  JSONìœ¼ë¡œ ì €ì¥í•˜ì—¬ ë©”íƒ€ë°ì´í„° ë³´ì¡´
    json_cache_path = os.path.join("parsed_cache", f"{file_hash}.json")
    
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # FAISS DB ë¡œë“œ
    if os.path.exists(faiss_cache_dir):
        if os.path.exists(os.path.join(faiss_cache_dir, "index.faiss")):
            st.info(f"ìºì‹œëœ ë²¡í„° DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (0.1ì´ˆ ì»·)")
            return FAISS.load_local(
                faiss_cache_dir, 
                embeddings, 
                allow_dangerous_deserialization=True
            )

    # íŒŒì‹± ë°ì´í„° ì¤€ë¹„
    llama_documents = []

    # 2. JSON ìºì‹œ í™•ì¸
    if os.path.exists(json_cache_path):
        st.info(f"íŒŒì‹±ëœ ë°ì´í„°(JSON)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (LlamaParse ì ˆì•½)")
        with open(json_cache_path, "r", encoding="utf-8") as f:
            cached_data = json.load(f)
            # JSON -> Document ê°ì²´ ë³µì›
            for item in cached_data:
                llama_documents.append(
                    Document(
                        page_content=item["text"],
                        metadata=item["metadata"]
                    )
                )
    else:
        # 3. LlamaParse ì‹¤í–‰
        try:
            st.info("LlamaCloudì—ì„œ ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (í† í° ì‚¬ìš©)")
            parser = LlamaParse(
                api_key=LLAMA_CLOUD_API_KEY,
                result_type="markdown",
                verbose=True
            )
            # LlamaParseëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•¨
            parsed_docs = parser.load_data(file_path)
            
            if not parsed_docs:
                return None
            
            llama_documents = parsed_docs

            # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)
            cache_data = []
            for doc in parsed_docs:
                cache_data.append({
                    "text": doc.text,
                    "metadata": doc.metadata # ì—¬ê¸°ì— page_labelì´ ë“¤ì–´ìˆìŒ
                })
            
            if not os.path.exists("parsed_cache"):
                os.makedirs("parsed_cache")
                
            with open(json_cache_path, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)

        except Exception as e:
            st.error(f"LlamaParse ì˜¤ë¥˜: {e}")
            return None

    # [Vector DB Build] ë©”íƒ€ë°ì´í„° ì •ë¦¬ ë° ì²­í‚¹
    st.info("ë°ì´í„°ë¥¼ ë²¡í„°í™” ì¤‘ì…ë‹ˆë‹¤...")
    
    langchain_documents = []
    
    for doc in llama_documents:
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        content = doc.text if hasattr(doc, 'text') else doc.page_content
        
        # 2. ë©”íƒ€ë°ì´í„° ì•ˆì „ ì¶”ì¶œ
        original_meta = doc.metadata if hasattr(doc, 'metadata') else {}
        
        # [í•µì‹¬] ì•ˆì „í•œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ í•¨ìˆ˜ ì‚¬ìš©
        raw_page_label = original_meta.get("page_label")
        raw_page_index = original_meta.get("page")
        
        # page_labelì´ ìˆìœ¼ë©´ ìš°ì„  ì“°ê³ , ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš©
        final_page_num = get_safe_page_number(raw_page_label, default=None)
        
        if final_page_num is None and raw_page_index is not None:
             final_page_num = int(raw_page_index) + 1 # 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1
        
        if final_page_num is None:
            final_page_num = 1 # ìµœí›„ì˜ ë³´ë£¨

        new_metadata = {
            "source": file_path,
            "page": final_page_num,     # ì´ì œ 'page'ëŠ” ë¬´ì¡°ê±´ ê¹¨ë—í•œ ì •ìˆ˜(int)
            "original_label": str(raw_page_label) # ì°¸ê³ ìš© ì›ë³¸
        }

        langchain_documents.append(
            Document(page_content=content, metadata=new_metadata)
        )

    # ì²­í‚¹ (Chunking) - ì´ì œ ìª¼ê°œì ¸ë„ 'page' ë©”íƒ€ë°ì´í„°ëŠ” ìœ ì§€ë¨!
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(langchain_documents)
    
    # FAISS ìƒì„± ë° ì €ì¥
    vectorstore = FAISS.from_documents(texts, embeddings)
    vectorstore.save_local(faiss_cache_dir)
    
    st.success("DB ìƒì„± ì™„ë£Œ!")
    return vectorstore

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# [ë©”ëª¨ë¦¬ ìµœì í™”] ConversationBufferMemoryëŠ” ì²´ì¸ ë‚´ë¶€ì—ì„œë§Œ ì“°ê³ ,
# UI í‘œì‹œëŠ” st.session_state.chat_historyë¡œ ê´€ë¦¬í•˜ì—¬ ì´ì¤‘ ì €ì¥ì„ ë°©ì§€í•˜ëŠ” íŒ¨í„´ ê¶Œì¥
# í•˜ì§€ë§Œ ì½”ë“œ ìˆ˜ì •ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê¸°ì¡´ êµ¬ì¡° ìœ ì§€í•˜ë˜, ë©”ëª¨ë¦¬ í‚¤ë¥¼ ëª…í™•íˆ í•¨
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

if "current_page" not in st.session_state:
    st.session_state.current_page = 1

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ë°ì´í„°ì‹œíŠ¸ PDF", type="pdf")

# ë©”ì¸ ë¡œì§
if uploaded_file is not None:
    # 1. íŒŒì¼ ì²˜ë¦¬
    binary_data = uploaded_file.getvalue()
    file_hash = get_file_hash(binary_data)
    file_path = f"temp_{file_hash}.pdf"
    
    with open(file_path, "wb") as f:
        f.write(binary_data)

    # 2. ë¡œë”©
    if "vectorstore" not in st.session_state or st.session_state.get("current_file_hash") != file_hash:
        vs = get_vectorstore(file_path, file_hash)
        if vs is None: st.stop()
        st.session_state.vectorstore = vs
        st.session_state.current_file_hash = file_hash

    # í™”ë©´ ë¶„í• 
    col1, col2 = st.columns([1, 1])

    # [Right] PDF Viewer
    with col2:
        st.info(f"ë¬¸ì„œ ë·°ì–´ (Page: {st.session_state.current_page})")
        pdf_viewer(
            input=binary_data,
            width=700,
            height=800,
            pages_to_render=[st.session_state.current_page],
            key="pdf_viewer"
        )

    # [Left] Chat
    with col1:
        st.subheader("ğŸ’¬ AI ì—”ì§€ë‹ˆì–´")
        chat_container = st.container(height=600)

        # ê¸°ë¡ ì¶œë ¥
        with chat_container:
            for message in st.session_state.chat_history:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

        # ì§ˆë¬¸ ì…ë ¥
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            # UI ì¦‰ì‹œ í‘œì‹œ
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(prompt)
            st.session_state.chat_history.append({"role": "user", "content": prompt})

            # ë‹µë³€ ìƒì„±
            with chat_container:
                with st.chat_message("assistant"):
                    with st.spinner("ë¶„ì„ ì¤‘..."):
                        llm = ChatGoogleGenerativeAI(
                            model="gemini-1.5-flash",
                            temperature=0,
                            max_retries=2
                        )
                        
                        qa_chain = ConversationalRetrievalChain.from_llm(
                            llm=llm,
                            retriever=st.session_state.vectorstore.as_retriever(),
                            memory=st.session_state.memory,
                            return_source_documents=True
                        )

                        result = qa_chain.invoke({"question": prompt})
                        response = result["answer"]
                        source_docs = result["source_documents"]

                        st.markdown(response)

                        # [í˜ì´ì§€ ì í”„ ë¡œì§ ê°œì„ ]
                        target_page = st.session_state.current_page
                        if source_docs:
                            # ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ 'page' ë©”íƒ€ë°ì´í„° ì‚¬ìš©
                            # (ìœ„ì—ì„œ ì´ë¯¸ intë¡œ ì •ì œí•´ë‘ )
                            doc_page = source_docs[0].metadata.get("page")
                            if doc_page:
                                target_page = int(doc_page)

                            # ê·¼ê±° í‘œì‹œ
                            with st.expander("ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš©"):
                                for doc in source_docs:
                                    p = doc.metadata.get("page")
                                    st.caption(f"[Page {p}] {doc.page_content[:200]}...")

            st.session_state.chat_history.append({"role": "assistant", "content": response})

            # Rerun
            if target_page != st.session_state.current_page:
                st.session_state.current_page = target_page
                st.rerun()